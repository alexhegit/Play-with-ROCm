{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c73b0caa-096b-45fe-b26f-032128d4334f",
   "metadata": {},
   "source": [
    "# Fine-tune Llama 2 with LoRA by AMD Radeon Pro W7900"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d94b31-35f8-4c8c-af0a-8a10aa5b4c62",
   "metadata": {},
   "source": [
    "In this blog, we show you how to fine-tune Llama 2 on one AMD Radeon Pro W7900 GPU(48GB GDDR) with ROCm. We use Low-Rank Adaptation of Large Language Models (LoRA) to overcome memory and computing limitations and make open-source large language models (LLMs) more accessible. We also show you how to fine-tune and upload models to Hugging Face.\n",
    "\n",
    "This blog is refer to https://rocm.blogs.amd.com/artificial-intelligence/llama2-lora/README.html which give some technical background about Llama2, Fine-tuning, LoRa and run the LoRA finetuning by AMD MI250 GPU. Here let's jump to the steps of fine-tuning by AMD Radeon Pro W7900 GPU.\n",
    "\n",
    "## Step-by-step Llama 2 fine-tuning\n",
    "\n",
    "Standard (full-parameter) fine-tuning involves considering all parameters. It requires significant computational power to manage optimizer states and gradient check-pointing. The resulting memory footprint is typically about four times larger than the model itself. For example, loading a 7 billion parameter model (e.g. Llama 2) in FP32 (4 bytes per parameter) requires approximately 28 GB of GPU memory, while fine-tuning demands around 28*4=112 GB of GPU memory. Note that the 112 GB figure is derived empirically, and various factors like batch size, data precision, and gradient accumulation contribute to overall memory usage.\n",
    "\n",
    "To overcome this memory limitation, you can use a parameter-efficient fine-tuning (PEFT) technique, such as LoRA.\n",
    "\n",
    "This example leverages tne AMD Radeon Pro W7900 GPU with 48GB VRAM. Using this setup allows us to explore different settings for fine-tuning the Llama 2–7b weights with LoRA.\n",
    "\n",
    "\n",
    "Our setup:\n",
    "\n",
    "- Hardware: AMD Radeon Pro W7900\n",
    "- Software:\n",
    "    - ROCm 6.0+\n",
    "    - Pytorch 2.0.1+\n",
    "\n",
    "Libraries: transformers, accelerate, peft, trl, bitsandbytes, scipy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4926a00e-7805-4de6-bb72-43db16ac09a2",
   "metadata": {},
   "source": [
    "### Step 0: Setup ROCm environment\n",
    "\n",
    "The easyway is to use ROCm docker image from https://hub.docker.com/r/rocm/pytorch. I use TAG rocm/pytorch:rocm6.1_ubuntu22.04_py3.10_pytorch_2.1.2.\n",
    "\n",
    "$docker pull rocm/pytorch:rocm6.1_ubuntu22.04_py3.10_pytorch_2.1.2\n",
    "\n",
    "And here is my docker start command as your reference.\n",
    "\n",
    "```\n",
    "$alias drun='docker run -it --network=host --device=/dev/kfd --device=/dev/dri/renderD128 --group-add=video --ipc=host --cap-add=SYS_PTRACE --security-opt seccomp=unconfined --shm-size 8G --hostname=w7900  -p 80:80 -p 8080:8080 -v /DATA:/DATA -w /DATA'\n",
    "\n",
    "$drun rocm/pytorch:rocm6.1_ubuntu22.04_py3.10_pytorch_2.1.2\n",
    "```\n",
    "\n",
    "To run this jupyter notebook, you may install it by `pip install jupyter-lab`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b72aff-616a-442c-91a2-b9974ab95255",
   "metadata": {},
   "source": [
    "### Step 1: Getting started\n",
    "\n",
    "First, let’s confirm the availability of the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f4a7d77-c810-44ba-b70d-4661139f1cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============================ ROCm System Management Interface ============================\n",
      "====================================== Product Info ======================================\n",
      "GPU[0]\t\t: Card series: \t\t0x7448\n",
      "GPU[0]\t\t: Card model: \t\t0x0e0d\n",
      "GPU[0]\t\t: Card vendor: \t\tAdvanced Micro Devices, Inc. [AMD/ATI]\n",
      "GPU[0]\t\t: Card SKU: \t\tD7070100\n",
      "==========================================================================================\n",
      "================================== End of ROCm SMI Log ===================================\n"
     ]
    }
   ],
   "source": [
    "!rocm-smi --showproductname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983ea9f6-1d12-4d99-9135-00d79e4dd1ef",
   "metadata": {},
   "source": [
    "Next, install the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41310ccd-6dd8-4222-81b4-143d79a377ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pandas peft==0.9.0 transformers==4.31.0 trl==0.4.7 accelerate scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e8dc50-0464-44c5-8931-0a6dfe812f79",
   "metadata": {},
   "source": [
    "#### Install bitsandbytes\n",
    "1. Install bitsandbytes using the following code.\n",
    "\n",
    "- For ROCm 6.2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dda99cff-bd74-4dbe-af9a-34d53001ef20",
   "metadata": {},
   "source": [
    "# Install `bitsandbytes`\n",
    "git clone --recurse https://github.com/ROCm/bitsandbytes.git\n",
    "cd bitsandbytes\n",
    "git checkout rocm6.2_internal_testing\n",
    "make hip\n",
    "python setup.py install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a0abf0-afad-4cc7-a46c-f832f107e1d7",
   "metadata": {},
   "source": [
    "2. Check the bitsandbytes version (0.42.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "948e866c-609e-492d-9a51-fd9760f68759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitsandbytes              0.42.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip list | grep bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb3982-f46e-47e5-b4f7-f9fbf873a2fc",
   "metadata": {},
   "source": [
    "#### Import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26886732-b369-495f-8b6b-decdf0564219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441d747b-4ef6-4969-9d15-f9834a5ee6bb",
   "metadata": {},
   "source": [
    "### Step 2: Configuring the model and data\n",
    "You can access Meta’s official Llama-2 model from Hugging Face after making a request, which can take a couple of days. Instead of waiting, we’ll use NousResearch’s Llama-2-7b-chat-hf as our base model (it’s the same as the original, but quicker to access). I downloaded it into /DATA/NousResearch/Llama-2-7b-chat-hf/ of my machine ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c916dcd-fc94-4214-895a-9720ad3ec3ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:21<00:00, 10.91s/it]\n"
     ]
    }
   ],
   "source": [
    "# Model and tokenizer names\n",
    "base_model_name = \"/DATA/NousResearch/Llama-2-7b-chat-hf/\"\n",
    "new_model_name = \"llama-2-7b-chat-enhanced\" #You can give your own name for fine tuned model\n",
    "\n",
    "# Tokenizer\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "80bac7fb-b5c4-4560-b3bd-6fdd1493b257",
   "metadata": {},
   "source": [
    "$ rocm-smi\n",
    "\n",
    "========================================== ROCm System Management Interface ==========================================\n",
    "==================================================== Concise Info ====================================================\n",
    "Device  [Model : Revision]    Temp    Power    Partitions      SCLK   MCLK     Fan    Perf  PwrCap       VRAM%  GPU%\n",
    "        Name (20 chars)       (Edge)  (Avg)    (Mem, Compute)\n",
    "======================================================================================================================\n",
    "0       [0x0e0d : 0x00]       38.0°C  29.0W    N/A, N/A        29Mhz  96Mhz    20.0%  auto  241.0W        57%   0%\n",
    "        0x7448\n",
    "======================================================================================================================\n",
    "================================================ End of ROCm SMI Log =================================================\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5211ad86-ab25-4618-bb08-f0666ed52bf9",
   "metadata": {},
   "source": [
    "After you have the base model, you can start fine-tuning. We fine-tune our base model for a question-and-answer task using a small data set called mlabonne/guanaco-llama2-1k, which is a subset (1,000 samples) of the timdettmers/openassistant-guanaco data set. This data set is a human-generated, human-annotated, assistant-style conversation corpus that contains 161,443 messages in 35 different languages, annotated with 461,292 quality ratings. This results in over 10,000 fully annotated conversation trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "235355d2-634d-4444-8497-71058a1e473a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1)\n",
      "{'text': '<s>[INST] write me a 1000 words essay about deez nuts. [/INST] The Deez Nuts meme first gained popularity in 2015 on the social media platform Vine. The video featured a young man named Rodney Bullard, who recorded himself asking people if they had heard of a particular rapper. When they responded that they had not, he would respond with the phrase \"Deez Nuts\" and film their reactions. The video quickly went viral, and the phrase became a popular meme. \\n\\nSince then, Deez Nuts has been used in a variety of contexts to interrupt conversations, derail discussions, or simply add humor to a situation. It has been used in internet memes, in popular music, and even in politics. In the 2016 US presidential election, a 15-year-old boy named Brady Olson registered as an independent candidate under the name Deez Nuts. He gained some traction in the polls and even made appearances on national news programs.\\n\\nThe Deez Nuts meme has had a significant impact on popular culture. It has become a recognizable catchphrase that people use to add humor to everyday conversations. The meme has also been used to satirize politics and other serious issues. For example, in 2016, a group of activists in the UK used the phrase \"Deez Nuts for President\" as part of a campaign to encourage young people to vote in the EU referendum. </s><s>[INST] Rewrite the essay in a more casual way. Instead of sounding proffesional, sound like a college student who is forced to write the essay but refuses to do so in the propper way. Use casual words and slang when possible. [/INST] Yo, so you want me to write a 1000-word essay about Deez Nuts? Alright, fine. So, this whole thing started on Vine back in 2015. Some dude named Rodney Bullard made a video where he would ask people if they knew a rapper, and when they said no, he would hit them with the classic line: \"Deez Nuts!\" People loved it, and it became a viral meme.\\n\\nNowadays, Deez Nuts is used for all kinds of stuff. You can throw it out there to interrupt someone or just to be funny. It\\'s all over the internet, in music, and even in politics. In fact, during the 2016 US presidential election, a kid named Brady Olson registered as an independent candidate under the name Deez Nuts. He actually got some attention from the media and made appearances on TV and everything.\\n\\nThe impact of Deez Nuts on our culture is pretty huge. It\\'s become a thing that everyone knows and uses to add some humor to their everyday conversations. Plus, people have used it to make fun of politics and serious issues too. Like, in the UK, some groups of activists used the phrase \"Deez Nuts for President\" to encourage young people to vote in the EU referendum.\\n\\nThere you have it, a thousand words about Deez Nuts in a more casual tone. Can I go back to playing video games now? </s>'}\n"
     ]
    }
   ],
   "source": [
    "# Data set\n",
    "data_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "training_data = load_dataset(data_name, split=\"train\")\n",
    "# check the data\n",
    "print(training_data.shape)\n",
    "# #11 is a QA sample in English\n",
    "print(training_data[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "076c8fa2-5b76-41ae-b5f3-b85e64faa368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboardX in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (2.6.2.2)\n",
      "Requirement already satisfied: protobuf>=3.20 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from tensorboardX) (3.20.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from tensorboardX) (24.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from tensorboardX) (1.22.4)\n"
     ]
    }
   ],
   "source": [
    "## There is a dependency during training\n",
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce556ea-9525-46ab-bcf2-1fe2e8c319d2",
   "metadata": {},
   "source": [
    "### Step 3: Start fine-tuning\n",
    "To set your training parameters, use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d525b641-c645-4987-9ae6-173d6a75e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Params\n",
    "train_params = TrainingArguments(\n",
    "    output_dir=\"./results_modified\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=50,\n",
    "    logging_steps=50,\n",
    "    learning_rate=4e-5,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f35f6b7-4a2f-467c-b1fd-e53bf7dcd837",
   "metadata": {},
   "source": [
    " I got OOM with per_device_train_batch_size=2 at AMD Radeon Pro W7900 with 48GB VRAM. You will see the VRAM usage bellow when run the LoRA finetuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d9d33c-95b2-467c-8212-96a7810b5e3c",
   "metadata": {},
   "source": [
    "#### Training with LoRA configuration\n",
    "Now you can integrate LoRA into the base model and assess its additional parameters. LoRA essentially adds pairs of rank-decomposition weight matrices (called update matrices) to existing weights, and only trains the newly added weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f3eecd4-a968-42dd-89e2-f94b4633ecb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "# LoRA Config\n",
    "peft_parameters = LoraConfig(\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(base_model, peft_parameters)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab9baef-c9a4-4438-aa88-cbcdcec199b9",
   "metadata": {},
   "source": [
    "Note that there are only 0.062% parameters added by LoRA, which is a tiny portion of the original model. This is the percentage we’ll update through fine-tuning, as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d21483ad-5ad7-43fd-87d1-2d383af73f7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 6006.52 examples/s]\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 16:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.954200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.778300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.559400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.496700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.345100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.408000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.402600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.336200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.300600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.176100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.219600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.377500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.306000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.356700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.313100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.265400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.347300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.298100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.386100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.319700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /DATA/NousResearch/Llama-2-7b-chat-hf/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /DATA/NousResearch/Llama-2-7b-chat-hf/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /DATA/NousResearch/Llama-2-7b-chat-hf/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /DATA/NousResearch/Llama-2-7b-chat-hf/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /DATA/NousResearch/Llama-2-7b-chat-hf/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /DATA/NousResearch/Llama-2-7b-chat-hf/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /DATA/NousResearch/Llama-2-7b-chat-hf/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /DATA/NousResearch/Llama-2-7b-chat-hf/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /DATA/NousResearch/Llama-2-7b-chat-hf/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /DATA/NousResearch/Llama-2-7b-chat-hf/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /DATA/NousResearch/Llama-2-7b-chat-hf/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /DATA/NousResearch/Llama-2-7b-chat-hf/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /DATA/NousResearch/Llama-2-7b-chat-hf/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /DATA/NousResearch/Llama-2-7b-chat-hf/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /DATA/NousResearch/Llama-2-7b-chat-hf/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /DATA/NousResearch/Llama-2-7b-chat-hf/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /DATA/NousResearch/Llama-2-7b-chat-hf/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /DATA/NousResearch/Llama-2-7b-chat-hf/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /DATA/NousResearch/Llama-2-7b-chat-hf/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /DATA/NousResearch/Llama-2-7b-chat-hf/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /DATA/NousResearch/Llama-2-7b-chat-hf/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=1.397334213256836, metrics={'train_runtime': 966.394, 'train_samples_per_second': 1.035, 'train_steps_per_second': 1.035, 'total_flos': 1.67211744380928e+16, 'train_loss': 1.397334213256836, 'epoch': 1.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainer with LoRA configuration\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=training_data,\n",
    "    peft_config=peft_parameters,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=llama_tokenizer,\n",
    "    args=train_params\n",
    ")\n",
    "\n",
    "# Training\n",
    "fine_tuning.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221a1781-648a-4902-b00e-096a71750d6f",
   "metadata": {},
   "source": [
    "The output looks like this:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f89b1c04-8c7d-4dd5-9961-1e020cd3a2d0",
   "metadata": {},
   "source": [
    "[250/250 07:59, Epoch 1/1]\\\n",
    "Step     Training Loss \\\n",
    "50       1.954200 \\\n",
    "100      1.778300\\\n",
    "150      1.559400\\\n",
    "200      1.496700\\\n",
    "250      1.345100\\\n",
    "...\n",
    "1000     1.319700\n",
    "\n",
    "TrainOutput(global_step=1000, training_loss=1.397334213256836, metrics={'train_runtime': 966.394, 'train_samples_per_second': 1.035, 'train_steps_per_second': 1.035, 'total_flos': 1.67211744380928e+16, 'train_loss': 1.397334213256836, 'epoch': 1.0})"
   ]
  },
  {
   "cell_type": "raw",
   "id": "38005e08-f1e8-4cde-a139-b18e74e41bb8",
   "metadata": {},
   "source": [
    "To save your model, run this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c0830b9-5f74-4d6f-bb33-355332c7e499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "fine_tuning.model.save_pretrained(new_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996f3834-0c06-4907-83cf-8f8ff9348f88",
   "metadata": {},
   "source": [
    "#### Checking memory usage during training with LoRA\n",
    "During training, you can check the memory usage by running the rocm-smi command in a terminal. This command produces the following output:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a05e31c-b5b5-4e58-ab2a-5b8004a3bec3",
   "metadata": {},
   "source": [
    "$ rocm-smi\n",
    "\n",
    "\n",
    "=========================================== ROCm System Management Interface ===========================================\n",
    "===================================================== Concise Info =====================================================\n",
    "Device  [Model : Revision]    Temp    Power   Partitions      SCLK     MCLK     Fan     Perf  PwrCap       VRAM%  GPU%\n",
    "        Name (20 chars)       (Edge)  (Avg)   (Mem, Compute)\n",
    "========================================================================================================================\n",
    "0       [0x0e0d : 0x00]       69.0°C  240.0W  N/A, N/A        1950Mhz  1124Mhz  40.78%  auto  241.0W        87%   99%\n",
    "        0x7448\n",
    "========================================================================================================================\n",
    "================================================= End of ROCm SMI Log ==================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42cf9ef-b529-4053-8f8e-f7cc80f34c33",
   "metadata": {},
   "source": [
    "To facilitate a comparison between fine-tuning with and without LoRA, our subsequent phase involves running a thorough fine-tuning process on the base model. This involves updating all parameters within the base model. We then analyze differences in memory usage, training speed, training loss, and other relevant metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eae7a3-39e1-42b6-bb21-10440e414314",
   "metadata": {},
   "source": [
    "#### Training without LoRA configuration\n",
    "\n",
    "You may got OOM failed of full-parameter fine-tunning process refer to https://rocm.blogs.amd.com/artificial-intelligence/llama2-lora/README.html. Yes, one Radeon Pro W7900 with 48GB VRAM is not enough for these case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7312872-12a7-4472-8111-a2ff43ad05b5",
   "metadata": {},
   "source": [
    "### Step 4: Test the fine-tuned model with LoRA\n",
    "\n",
    "To test your model, run the following code:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d6af907-b55b-4e5f-9b75-519fdd0c5ee0",
   "metadata": {},
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "model = PeftModel.from_pretrained(base_model, new_model_name)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da05da4-faf6-4538-8894-30a9dd0bb427",
   "metadata": {},
   "source": [
    "The output looks like this:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b7789dfa-f685-4c9d-9ecb-66a88d1e2705",
   "metadata": {},
   "source": [
    "    Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.34s/it]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f259219a-c28d-4404-aca1-85a61816cbb5",
   "metadata": {},
   "source": [
    "Uploading the model to Hugging Face let’s you conduct subsequent tests or share your model with others (to proceed with this step, you’ll need an active Hugging Face account)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a4cb5c0-0db9-4459-bfbe-91cfb1dedce6",
   "metadata": {},
   "source": [
    "from huggingface_hub import login\n",
    "# You need to use your Hugging Face Access Tokens\n",
    "login(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n",
    "# Push the model to Hugging Face. This takes minutes and time depends the model size and your\n",
    "# network speed.\n",
    "model.push_to_hub(new_model_name, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(new_model_name, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcfc34f-14e5-4aa2-b820-a428a993da06",
   "metadata": {},
   "source": [
    "Now you can test with the base model (original) and your fine-tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd52a8df-4841-4b0f-80ba-e868e2a76db0",
   "metadata": {},
   "source": [
    "- Base model:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fdf6b408-deca-4e08-833c-c860e2d6b76c",
   "metadata": {},
   "source": [
    "# Generate text using base model\n",
    "query = \"What do you think is the most important part of building an AI chatbot?\"\n",
    "text_gen = pipeline(task=\"text-generation\", model=base_model_name, tokenizer=llama_tokenizer, max_length=200)\n",
    "output = text_gen(f\"<s>[INST] {query} [/INST]\")\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb8190f-4c40-48ed-baee-525472f965f0",
   "metadata": {},
   "source": [
    "- Fine-tuned model:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f729432f-d499-4878-aac7-bfea303ca6d6",
   "metadata": {},
   "source": [
    "# Generate text using fine-tuned model\n",
    "query = \"What do you think is the most important part of building an AI chatbot?\"\n",
    "text_gen = pipeline(task=\"text-generation\", model=new_model_name, tokenizer=llama_tokenizer, max_length=200)\n",
    "output = text_gen(f\"<s>[INST] {query} [/INST]\")\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fb081a-cf75-4f02-89d4-74e12124dc36",
   "metadata": {},
   "source": [
    "You can observe the outputs of the two models based on a given query. These outputs exhibit slight differences due to the fine-tuning process altering the model weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
