{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c73b0caa-096b-45fe-b26f-032128d4334f",
   "metadata": {},
   "source": [
    "# Fine-Tuning Llama-3.1 with QLoRA on AMD ROCm GPUs\n",
    "\n",
    "This tutorial demonstrates how to fine-tune the **Llama-3.1-8B** large language model using **Quantized Low-Rank Adaptation (QLoRA)** on AMD ROCm GPUs. **Llama-3.1**, developed by Meta, is a widely used open-source large language model. For more information, visit [Meta's Llama page](https://ai.meta.com/llama/).\n",
    "\n",
    "**QLoRA**, introduced by Dettmers et al. in [their 2023 paper](https://arxiv.org/abs/2305.14314), is a parameter-efficient fine-tuning technique that combines quantization and low-rank adaptation to enable efficient fine-tuning of large models with minimal resource requirements.\n",
    "\n",
    "> **Reference**: Dettmers et al., \"QLoRA: Efficient Finetuning of Quantized LLMs,\" 2023.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d94b31-35f8-4c8c-af0a-8a10aa5b4c62",
   "metadata": {},
   "source": [
    "\n",
    "## **Prerequisites**\n",
    "\n",
    "### **1. Hardware Requirements**\n",
    "- AMD Inistict GPUs (e.g., MI210, MI300X) and Radeon GPU (e.g. Radeon Pro W7900)\n",
    "- It should need multiple GPUs for some big size LLM. This jupyter notebook could run LLama-3.1-8B QLoRA Finetuning on one Radeon Pro W7900.\n",
    "- Ensure your system meets the [System Requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html), including ROCm 6.0+ and Ubuntu 22.04.\n",
    "\n",
    "### **2. Docker**\n",
    "- Install Docker with GPU support.\n",
    "- Ensure your user has appropriate permissions to access the GPU.\n",
    "- Verify Docker permissions and GPU access:\n",
    "  ```bash\n",
    "  docker run --rm --device=/dev/kfd --device=/dev/dri rocm/pytorch:rocm6.2.3_ubuntu22.04_py3.10_pytorch_release_2.3.0\n",
    "  ```\n",
    "\n",
    "### **3. Hugging Face API Access**\n",
    "- Obtain an API token from [Hugging Face](https://huggingface.co) for downloading models.\n",
    "- Ensure you have a Hugging Face API token with the necessary permissions and approval to access [Meta's LLaMA checkpoints](https://huggingface.co/meta-llama/Llama-3.1-8B).\n",
    "\n",
    "### **4. Data Preparation**\n",
    "- For this tutorial, we use a sample dataset from Hugging Face, which will be prepared during the setup steps.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4926a00e-7805-4de6-bb72-43db16ac09a2",
   "metadata": {},
   "source": [
    "## **Prepare Training Environment**\n",
    "\n",
    "### **1. Pull the Docker Image**\n",
    "\n",
    "Ensure your system meets the [System Requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html).\n",
    "\n",
    "Pull the Docker image required for this tutorial:\n",
    "\n",
    "```bash\n",
    "docker pull rocm/pytorch:rocm6.2.3_ubuntu22.04_py3.10_pytorch_release_2.3.0\n",
    "```\n",
    "\n",
    "### **2. Launch the Docker Container**\n",
    "\n",
    "Launch the Docker container and map the necessary directories. Replace `/path/to/notebooks` with the full path to the directory on your host machine where these notebooks are stored.\n",
    "\n",
    "```bash\n",
    "docker run -it --rm \\\n",
    "  --network=host \\\n",
    "  --device=/dev/kfd \\\n",
    "  --device=/dev/dri \\\n",
    "  --group-add=video \\\n",
    "  --ipc=host \\\n",
    "  --cap-add=SYS_PTRACE \\\n",
    "  --security-opt seccomp=unconfined \\\n",
    "  --shm-size 8G \\\n",
    "  --hostname=ROCm-FT \\\n",
    "  -v /path/to/notebooks:/workspace/notebooks \\\n",
    "  -w /workspace/notebooks \\\n",
    "  rocm/pytorch:rocm6.2.3_ubuntu22.04_py3.10_pytorch_release_2.3.0\n",
    "```\n",
    "\n",
    "**Important**: Replace `/path/to/notebooks` with the absolute path to the directory on your host machine where your notebooks are stored. Ensure this directory is accessible to Docker and contains the necessary files for this tutorial.\n",
    "\n",
    "### **3. Install and Launch Jupyter**\n",
    "\n",
    "Inside the Docker container, install Jupyter using the following command:\n",
    "\n",
    "```bash\n",
    "pip install --upgrade pip setuptools wheel\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "Start the Jupyter server:\n",
    "```bash\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b3698c",
   "metadata": {},
   "source": [
    "### **4. Install Required Libraries**\n",
    "Install the libraries needed for this tutorial. Run the following commands inside the Jupyter notebook running within the Docker container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10a51315",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: peft==0.14.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (0.14.0)\n",
      "Requirement already satisfied: transformers==4.47.1 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (4.47.1)\n",
      "Requirement already satisfied: trl==0.13.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (0.13.0)\n",
      "Requirement already satisfied: accelerate==1.2.1 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (1.2.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (1.14.1)\n",
      "Requirement already satisfied: tensorboardX in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (2.6.2.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from peft==0.14.0) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from peft==0.14.0) (24.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from peft==0.14.0) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from peft==0.14.0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from peft==0.14.0) (2.3.0a0+gitd2f9472)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from peft==0.14.0) (4.66.5)\n",
      "Requirement already satisfied: safetensors in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from peft==0.14.0) (0.5.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from peft==0.14.0) (0.27.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from transformers==4.47.1) (3.16.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from transformers==4.47.1) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from transformers==4.47.1) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from transformers==4.47.1) (0.21.0)\n",
      "Requirement already satisfied: datasets>=2.21.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from trl==0.13.0) (3.2.0)\n",
      "Requirement already satisfied: rich in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from trl==0.13.0) (13.9.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: protobuf>=3.20 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from tensorboardX) (3.20.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from datasets>=2.21.0->trl==0.13.0) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from datasets>=2.21.0->trl==0.13.0) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from datasets>=2.21.0->trl==0.13.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from datasets>=2.21.0->trl==0.13.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages/fsspec-2024.9.0-py3.10.egg (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.21.0->trl==0.13.0) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from datasets>=2.21.0->trl==0.13.0) (3.10.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft==0.14.0) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from requests->transformers==4.47.1) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from requests->transformers==4.47.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from requests->transformers==4.47.1) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from requests->transformers==4.47.1) (2024.8.30)\n",
      "Requirement already satisfied: sympy<=1.12.1 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.14.0) (1.12.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.14.0) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.14.0) (3.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from rich->trl==0.13.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from rich->trl==0.13.0) (2.15.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl==0.13.0) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl==0.13.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl==0.13.0) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl==0.13.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl==0.13.0) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl==0.13.0) (1.14.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl==0.13.0) (4.0.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl==0.13.0) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from sympy<=1.12.1->torch>=1.13.0->peft==0.14.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.14.0) (3.0.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.21.0->trl==0.13.0) (0.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries for fine-tuning, including parameter-efficient fine-tuning (peft) and transformers\n",
    "!pip install pandas peft==0.14.0 transformers==4.47.1 trl==0.13.0 accelerate==1.2.1 scipy tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f703213",
   "metadata": {},
   "source": [
    "Verify the installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6cfc30b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft                      0.14.0\n",
      "transformers              4.47.1\n",
      "accelerate                1.2.1\n",
      "trl                       0.13.0\n"
     ]
    }
   ],
   "source": [
    "# Verify the installation and version of the required libraries\n",
    "!pip list | grep peft\n",
    "!pip list | grep transformer\n",
    "!pip list | grep accelerate\n",
    "!pip list | grep trl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea665d8",
   "metadata": {},
   "source": [
    "### **5. Install BitsAndBytes (ROCm 6.2)**\n",
    "Install bitsandbytes from source for ROCm 6.2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0627255-a9e5-4805-b330-318a460943e1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Install bitsandbytes from source to enable quantization on ROCm GPUs\n",
    "!git clone --recurse https://github.com/ROCm/bitsandbytes.git && cd bitsandbytes && git checkout rocm6.2_internal_testing && make hip && python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6d42550-fbe4-4afb-a38c-04a9103f78cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'bitsandbytes' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone --recurse https://github.com/ROCm/bitsandbytes.git && cd bitsandbytes && git checkout rocm_enabled_multi_backend && pip install -r requirements-dev.txt && cmake -DCOMPUTE_BACKEND=hip -S . && make && pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0aa46a",
   "metadata": {},
   "source": [
    "Verify the installation (version 0.43.3.dev):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2dfa6d3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Copyright (C) 2021 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n",
      "bitsandbytes version: 0.43.3.dev\n"
     ]
    }
   ],
   "source": [
    "# Verify the installation and version of bitsandbytes\n",
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "    print(\"bitsandbytes version:\", bnb.__version__)\n",
    "except ImportError as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fe340b5-680b-4eba-a806-a85f8057cd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitsandbytes              0.43.3.dev0\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7d0c48",
   "metadata": {},
   "source": [
    "\n",
    "**⚠️ Important: Ensure the Correct Kernel is Selected**  \n",
    "If this process fails, please ensure the correct Jupyter kernel is selected for your notebook.\n",
    "To do this:\n",
    "1. Go to the \"Kernel\" menu.\n",
    "2. Click \"Change Kernel.\"\n",
    "3. Select `Python 3 (ipykernel)` from the list.\n",
    "\n",
    "**Failure to select the correct kernel may lead to unexpected issues when running the notebook.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fad2378",
   "metadata": {},
   "source": [
    "### **6. Provide Your Hugging Face Token**\n",
    "\n",
    "You will need a Hugging Face API token to access Llama-3.1-8B. Tokens typically start with \"hf_\". Generate your token at [Hugging Face Tokens](https://huggingface.co/settings/tokens) and request access for [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B).\n",
    "\n",
    "Run the following interactive block in your Jupyter notebook to set up the token:\n",
    "\n",
    "***Note***: Please uncheck the \"Add token as Git credential\" option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc98d94d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa13262fa2542c590a7c065b5b9a835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login, HfApi\n",
    "\n",
    "# Prompt the user to log in\n",
    "status = notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c363bf3",
   "metadata": {},
   "source": [
    "Verify that your token was captured correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "828fbef7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token validated successfully! Logged in as: alexhegit\n"
     ]
    }
   ],
   "source": [
    "# Validate the token\n",
    "try:\n",
    "    api = HfApi()\n",
    "    user_info = api.whoami()\n",
    "    print(f\"Token validated successfully! Logged in as: {user_info['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Token validation failed. Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cb55cf-7f2d-45c6-9c5c-86a82ca4c9c6",
   "metadata": {},
   "source": [
    "## Fine-Tuning the Model\n",
    "\n",
    "This section walks through the process of setting up and executing fine-tuning for the Llama-3.1 model using the QLoRA technique. The following steps include setting up GPUs, importing the required libraries, configuring the model and training parameters, and running the fine-tuning process.\n",
    "\n",
    "### Set and Verify GPU Availability\n",
    "\n",
    "Begin by specifying the GPUs available for fine-tuning and verifying that they are properly detected by PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27fb94e7-c059-4883-97dc-c36546e65236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch detected number of available devices: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "gpus = [0, 1, 2, 3] # Specify the GPUs to be used for training\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", ','.join(map(str, gpus)))\n",
    "# Ensure PyTorch detects the GPUs correctly\n",
    "print(f\"PyTorch detected number of available devices: {torch.cuda.device_count()}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb3982-f46e-47e5-b4f7-f9fbf873a2fc",
   "metadata": {},
   "source": [
    "### Import the Required Packages\n",
    "\n",
    "Next, import the libraries necessary for fine-tuning, including utilities for dataset loading, model configuration, training setup, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26886732-b369-495f-8b6b-decdf0564219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported required libraries for dataset handling, model configuration, and QLoRA fine-tuning.\n"
     ]
    }
   ],
   "source": [
    "# Load datasets and transformers for handling the Llama-3.1 model\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "# Import utilities for QLoRA fine-tuning and training configurations\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "print(\"Successfully imported required libraries for dataset handling, model configuration, and QLoRA fine-tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441d747b-4ef6-4969-9d15-f9834a5ee6bb",
   "metadata": {},
   "source": [
    "### Configuring the Model \n",
    "\n",
    "Load the base model, tokenizer, and set up the quantization configuration for efficient fine-tuning on ROCm-enabled GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c916dcd-fc94-4214-895a-9720ad3ec3ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_model_name = \"meta-llama/Llama-3.1-8B\"  # Hugging Face model repository name\n",
    "new_model_name = \"Llama-3.1-8B-qlora\"  # Name for the fine-tuned model\n",
    "\n",
    "# Load and configure the tokenizer for padding and tokenization\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_name, \n",
    "    trust_remote_code=True, \n",
    "    use_fast=True\n",
    ")\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8f28c2",
   "metadata": {},
   "source": [
    "### Quantization Configuration in QLoRA\n",
    "\n",
    "As outlined in the [QLoRA paper](https://arxiv.org/abs/2305.14314), weights are stored in a 4-bit format, allowing computations to occur in 16-bit or 32-bit precision. When a QLoRA weight tensor is used, it is dequantized to the chosen precision (16-bit or 32-bit) before performing matrix multiplication. Various precision combinations, such as `float16`, `bfloat16`, and `float32`, are supported. You can experiment with different 4-bit quantization methods, including **NormalFloat4 (NF4)** and pure `float4`. However, based on theoretical insights and empirical results from the paper, NF4 is recommended for its superior performance.\n",
    "\n",
    "For this tutorial, we use the following configuration:\n",
    "\n",
    "- **4-bit quantization** with the NF4 type.  \n",
    "- **16-bit (float16)** precision for computations.  \n",
    "- **Double quantization**, which applies a second quantization step to reduce memory usage by an additional 0.3 bits per parameter.  \n",
    "\n",
    "These quantization parameters are controlled using the `BitsandbytesConfig` (refer to [Hugging Face documentation](https://huggingface.co/docs)) as follows:\n",
    "\n",
    "- **`load_in_4bit`**: Activates loading the model in 4-bit precision.  \n",
    "- **`bnb_4bit_quant_type`**: Specifies the quantization type, with options for `fp4` (four-bit float) and `nf4` (normal four-bit float). As NF4 is optimized for normally distributed weights, it is the recommended choice.  \n",
    "- **`bnb_4bit_compute_dtype`**: Determines the data type used for linear layer computations.  \n",
    "- **`bnb_4bit_use_double_quant`**: Activates double quantization for further memory optimization.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ae3c4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abde8d4265ae46bba3c02ea8774e2cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Configure 4-bit quantization to reduce memory usage\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_compute_dtype=\"float16\", \n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# Load the pre-trained Llama-3.1 model with device mapping for GPU\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Disable caching to optimize for fine-tuning\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe90d81-e99e-46fd-bf96-5246210f75df",
   "metadata": {},
   "source": [
    "### Load and Prepare the Dataset\n",
    "\n",
    "Fine-tune the base model for a question-and-answer task using a small dataset called [mlabonne/guanaco-llama2-1k](https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k/tree/main). This dataset is a subset (1,000 samples) of the [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) dataset. This dataset is a human-generated, human-annotated, assistant-style conversation corpus that contains 161,443 messages in 35 different languages, annotated with 461,292 quality ratings. This results in over 10,000 fully annotated conversation trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "235355d2-634d-4444-8497-71058a1e473a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1)\n",
      "{'text': '<s>[INST] write me a 1000 words essay about deez nuts. [/INST] The Deez Nuts meme first gained popularity in 2015 on the social media platform Vine. The video featured a young man named Rodney Bullard, who recorded himself asking people if they had heard of a particular rapper. When they responded that they had not, he would respond with the phrase \"Deez Nuts\" and film their reactions. The video quickly went viral, and the phrase became a popular meme. \\n\\nSince then, Deez Nuts has been used in a variety of contexts to interrupt conversations, derail discussions, or simply add humor to a situation. It has been used in internet memes, in popular music, and even in politics. In the 2016 US presidential election, a 15-year-old boy named Brady Olson registered as an independent candidate under the name Deez Nuts. He gained some traction in the polls and even made appearances on national news programs.\\n\\nThe Deez Nuts meme has had a significant impact on popular culture. It has become a recognizable catchphrase that people use to add humor to everyday conversations. The meme has also been used to satirize politics and other serious issues. For example, in 2016, a group of activists in the UK used the phrase \"Deez Nuts for President\" as part of a campaign to encourage young people to vote in the EU referendum. </s><s>[INST] Rewrite the essay in a more casual way. Instead of sounding proffesional, sound like a college student who is forced to write the essay but refuses to do so in the propper way. Use casual words and slang when possible. [/INST] Yo, so you want me to write a 1000-word essay about Deez Nuts? Alright, fine. So, this whole thing started on Vine back in 2015. Some dude named Rodney Bullard made a video where he would ask people if they knew a rapper, and when they said no, he would hit them with the classic line: \"Deez Nuts!\" People loved it, and it became a viral meme.\\n\\nNowadays, Deez Nuts is used for all kinds of stuff. You can throw it out there to interrupt someone or just to be funny. It\\'s all over the internet, in music, and even in politics. In fact, during the 2016 US presidential election, a kid named Brady Olson registered as an independent candidate under the name Deez Nuts. He actually got some attention from the media and made appearances on TV and everything.\\n\\nThe impact of Deez Nuts on our culture is pretty huge. It\\'s become a thing that everyone knows and uses to add some humor to their everyday conversations. Plus, people have used it to make fun of politics and serious issues too. Like, in the UK, some groups of activists used the phrase \"Deez Nuts for President\" to encourage young people to vote in the EU referendum.\\n\\nThere you have it, a thousand words about Deez Nuts in a more casual tone. Can I go back to playing video games now? </s>'}\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "data_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "# Load the fine-tuning dataset from Hugging Face\n",
    "training_data = load_dataset(data_name, split=\"train\")\n",
    "\n",
    "# Display dataset structure and a sample for verification\n",
    "print(training_data.shape)\n",
    "#11 is a QA sample in English\n",
    "print(training_data[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce556ea-9525-46ab-bcf2-1fe2e8c319d2",
   "metadata": {},
   "source": [
    "### Fine-Tuning Configuration\n",
    "\n",
    "Define the hyperparameters and configurations for the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d525b641-c645-4987-9ae6-173d6a75e292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training parameters configured!.\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments, including output directory and optimization settings\n",
    "# Specify number of epochs, batch size, learning rate, and logging steps\n",
    "train_params = TrainingArguments(\n",
    "    output_dir=\"./results_qlora\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=50,\n",
    "    logging_steps=50,\n",
    "    learning_rate=4e-5,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "print(\"Training parameters configured!.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f35f6b7-4a2f-467c-b1fd-e53bf7dcd837",
   "metadata": {},
   "source": [
    "***NOTE***：If you encounter out-of-memory (OOM) errors, reduce per_device_train_batch_size or enable gradient checkpointing. Use rocm-smi to monitor VRAM usage during fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d9d33c-95b2-467c-8212-96a7810b5e3c",
   "metadata": {},
   "source": [
    "### QLoRA Configuration\n",
    "\n",
    "Low-Rank Adaptation (QLoRA) introduces lightweight rank-decomposition matrices into the base model. By focusing only on updating these additional matrices, QLoRA reduces the number of trainable parameters significantly, enabling efficient fine-tuning of large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f3eecd4-a968-42dd-89e2-f94b4633ecb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "# Configure QLoRA parameters for low-rank adaptation\n",
    "peft_parameters = LoraConfig(\n",
    "    lora_alpha=8, # Alpha controls the scaling parameter\n",
    "    lora_dropout=0.1,\n",
    "    r=8, # r specifies the rank of the low-rank matrices\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(base_model, peft_parameters)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab9baef-c9a4-4438-aa88-cbcdcec199b9",
   "metadata": {},
   "source": [
    "Expected Output:\n",
    "```\n",
    "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n",
    "```\n",
    "This indicates that only 0.042% of the total parameters are trainable during fine-tuning, which is a tiny fraction of the overall model, ensuring resource efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd32708",
   "metadata": {},
   "source": [
    "### Fine-Tuning with QLoRA\n",
    "QLoRA's lightweight approach allows fine-tuning while maintaining high efficiency in terms of computation and memory usage. We now define a training pipeline using the QLoRA-integrated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "444a176a-a893-46ee-8df9-b9bc4ddca1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:566: UserWarning: 1Torch was not compiled with memory efficient attention. (Triggered internally at /var/lib/jenkins/pytorch/aten/src/ATen/native/transformers/hip/sdp_utils.cpp:517.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 19:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.680500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.544300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.466000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.358200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.306900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.363900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.400100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.368900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.312900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.276800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.287400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.399100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.394200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.438100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.375600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.273100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.369200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.346200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.387100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.379400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=1.3864017639160156, metrics={'train_runtime': 1154.1898, 'train_samples_per_second': 0.866, 'train_steps_per_second': 0.866, 'total_flos': 1.6503795464994816e+16, 'train_loss': 1.3864017639160156, 'epoch': 1.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the trainer with the fine-tuning dataset and configurations\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=training_data,\n",
    "    peft_config=peft_parameters,\n",
    "    args=train_params\n",
    ")\n",
    "\n",
    "# Execute the training process\n",
    "fine_tuning.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52620657",
   "metadata": {},
   "source": [
    "During training, the model outputs metrics such as training loss, step progress, and runtime performance, which can be monitored for insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38005e08-f1e8-4cde-a139-b18e74e41bb8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Save the Fine-Tuned Model\n",
    "\n",
    "After training is complete, save the model with the specified name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c0830b9-5f74-4d6f-bb33-355332c7e499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved the model!\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model to the specified directory\n",
    "fine_tuning.model.save_pretrained(new_model_name)\n",
    "print(\"Successfully saved the model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996f3834-0c06-4907-83cf-8f8ff9348f88",
   "metadata": {},
   "source": [
    "### Monitoring GPU Memory\n",
    "\n",
    "To monitor GPU memory during training, use the following command in a terminal:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a05e31c-b5b5-4e58-ab2a-5b8004a3bec3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "!rocm-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91846614",
   "metadata": {},
   "source": [
    "This will display memory usage and other GPU metrics to ensure your hardware resources are used optimally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42cf9ef-b529-4053-8f8e-f7cc80f34c33",
   "metadata": {},
   "source": [
    "### Comparison: Fine-Tuning with and without QLoRA\n",
    "\n",
    "To understand the benefits of QLoRA, you can compare fine-tuning metrics (such as memory usage, training speed, and loss) between:\n",
    "\n",
    "- Fine-tuning with QLoRA.\n",
    "- fine-tuning.\n",
    "\n",
    "QLoRA's resource-efficient approach is especially beneficial for training on hardware with limited memory or computational power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7312872-12a7-4472-8111-a2ff43ad05b5",
   "metadata": {},
   "source": [
    "### Testing the Fine-Tuned Model\n",
    "\n",
    "Load the fine-tuned model and run inference to evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0451b4ef-24b8-419e-816f-33eaac534dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e78de22d0641dca087b00fe13185e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Reload model in FP16 and merge it with QLoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "from peft import LoraConfig, PeftModel\n",
    "peft_model = PeftModel.from_pretrained(base_model, new_model_name)\n",
    "peft_model = peft_model.merge_and_unload()\n",
    "\n",
    "# Configure the tokenizer for text generation\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\"\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=peft_model, \n",
    "    tokenizer=llama_tokenizer,\n",
    "    max_length=1024,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a9071a",
   "metadata": {},
   "source": [
    "Now let's run a query and view the response generated by our fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d36335c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] What do you think is the most important part of building an AI chatbot? [/INST] There are several key components that are important for building an AI chatbot. These include:\n",
      "\n",
      "1. Natural Language Processing (NLP): NLP is the ability of a computer program to understand and process human language. This is essential for a chatbot to be able to understand and respond to user input.\n",
      "\n",
      "2. Machine Learning: Machine learning algorithms are used to train the chatbot on a large dataset of conversations and to improve its ability to understand and respond to user input over time.\n",
      "\n",
      "3. Dialog management: Dialog management is the process of managing the conversation between the user and the chatbot. This includes understanding the user's intent, maintaining context, and generating appropriate responses.\n",
      "\n",
      "4. User experience: The user experience is an important factor in the success of a chatbot. This includes the design of the interface, the speed and accuracy of the responses, and the overall usability of the chatbot.\n",
      "\n",
      "Overall, a successful AI chatbot requires a combination of these components, as well as a deep understanding of the target audience and their needs. </s>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use the fine-tuned model to generate responses for a query\n",
    "query = \"What do you think is the most important part of building an AI chatbot?\"\n",
    "output = pipeline(f\"<s>[INST] {query} [/INST]\")\n",
    "print(output[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
