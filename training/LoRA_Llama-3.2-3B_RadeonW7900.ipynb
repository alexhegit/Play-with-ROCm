{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c73b0caa-096b-45fe-b26f-032128d4334f",
   "metadata": {},
   "source": [
    "# Fine-Tuning Llama-3.2 with LoRA on AMD ROCm GPUs\n",
    "\n",
    "This tutorial demonstrates how to fine-tune the **Llama-3.2-3B** large language model using **Low-Rank Adaptation (LoRA)** on AMD ROCm GPUs. **Llama-3.2**, developed by Meta, is a widely used open-source large language model. For more information, visit [Meta's Llama page](https://ai.meta.com/llama/).\n",
    "\n",
    "Fine-tuning large language models can be computationally intensive due to the need to optimize all parameters. This approach, known as **full-parameter fine-tuning**, requires updating every weight in the model, leading to significant demand of memory and compute resources, often up to four times the size of the model itself.\n",
    "\n",
    "To address these challenges, we use **LoRA** (Low-Rank Adaptation), a parameter-efficient fine-tuning (PEFT) technique. As described by Hu et al. in [their 2021 paper](https://arxiv.org/abs/2106.09685), LoRA freezes the pre-trained model weights and introduces trainable rank-decomposition matrices into each layer of the Transformer architecture. This significantly reduces the number of trainable parameters for downstream tasks while maintaining performance, making it possible to fine-tune large models efficiently on resource-constrained hardware.\n",
    "\n",
    "> **Reference**: Hu et al., \"LoRA: Low-Rank Adaptation of Large Language Models,\" 2021.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d94b31-35f8-4c8c-af0a-8a10aa5b4c62",
   "metadata": {},
   "source": [
    "\n",
    "## **Prerequisites**\n",
    "\n",
    "### **1. Hardware Requirements**\n",
    "- AMD Instict GPUs (e.g., MI210, MI300X) and Radeon GPU (e.g. Radeon Pro W7900) \n",
    "- Ensure your system meets the [System Requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html), including ROCm 6.0+ and Ubuntu 22.04.\n",
    "\n",
    "### **2. Docker**\n",
    "- Install Docker with GPU support.\n",
    "- Ensure your user has appropriate permissions to access the GPU.\n",
    "- Verify Docker permissions and GPU access:\n",
    "  ```bash\n",
    "  docker run --rm --device=/dev/kfd --device=/dev/dri rocm/pytorch:rocm6.2.3_ubuntu22.04_py3.10_pytorch_release_2.3.0\n",
    "  ```\n",
    "\n",
    "### **3. Hugging Face API Access**\n",
    "- Obtain an API token from [Hugging Face](https://huggingface.co) for downloading models.\n",
    "- Ensure you have a Hugging Face API token with the necessary permissions and approval to access [Meta's LLaMA checkpoints](https://huggingface.co/meta-llama/Llama-3.2-3B).\n",
    "\n",
    "### **4. Data Preparation**\n",
    "- For this tutorial, we use a sample dataset from Hugging Face, which will be prepared during the setup steps.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4926a00e-7805-4de6-bb72-43db16ac09a2",
   "metadata": {},
   "source": [
    "## **Prepare Training Environment**\n",
    "\n",
    "### **1. Pull the Docker Image**\n",
    "\n",
    "Ensure your system meets the [System Requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html).\n",
    "\n",
    "Pull the Docker image required for this tutorial:\n",
    "\n",
    "```bash\n",
    "docker pull rocm/pytorch:rocm6.2.3_ubuntu22.04_py3.10_pytorch_release_2.3.0\n",
    "```\n",
    "\n",
    "### **2. Launch the Docker Container**\n",
    "\n",
    "Launch the Docker container and map the necessary directories. Replace `/path/to/notebooks` with the full path to the directory on your host machine where these notebooks are stored.\n",
    "\n",
    "```bash\n",
    "docker run -it --rm \\\n",
    "  --network=host \\\n",
    "  --device=/dev/kfd \\\n",
    "  --device=/dev/dri \\\n",
    "  --group-add=video \\\n",
    "  --ipc=host \\\n",
    "  --cap-add=SYS_PTRACE \\\n",
    "  --security-opt seccomp=unconfined \\\n",
    "  --shm-size 8G \\\n",
    "  --hostname=ROCm-FT \\\n",
    "  -v /path/to/notebooks:/workspace/notebooks \\\n",
    "  -w /workspace/notebooks \\\n",
    "  rocm/pytorch:rocm6.2.3_ubuntu22.04_py3.10_pytorch_release_2.3.0\n",
    "```\n",
    "\n",
    "**Important**: Replace `/path/to/notebooks` with the absolute path to the directory on your host machine where your notebooks are stored. Ensure this directory is accessible to Docker and contains the necessary files for this tutorial.\n",
    "\n",
    "### **3. Install and Launch Jupyter**\n",
    "\n",
    "Inside the Docker container, install Jupyter using the following command:\n",
    "\n",
    "```bash\n",
    "pip install --upgrade pip setuptools wheel\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "Start the Jupyter server:\n",
    "```bash\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b3698c",
   "metadata": {},
   "source": [
    "### **4. Install Required Libraries**\n",
    "Install the libraries needed for this tutorial. Run the following commands inside the Jupyter notebook running within the Docker container:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "189865b6-10e0-4c64-8664-3308a02f65b6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Install necessary libraries for fine-tuning, including parameter-efficient fine-tuning (peft) and transformers\n",
    "!pip install pandas peft==0.14.0 transformers==4.47.1 trl==0.13.0 accelerate==1.2.1 scipy tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f703213",
   "metadata": {},
   "source": [
    "Verify the installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6cfc30b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft                      0.14.0\n",
      "transformers              4.47.1\n",
      "accelerate                1.2.1\n",
      "trl                       0.13.0\n"
     ]
    }
   ],
   "source": [
    "# Verify the installation and version of the required libraries\n",
    "!pip list | grep peft\n",
    "!pip list | grep transformer\n",
    "!pip list | grep accelerate\n",
    "!pip list | grep trl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fad2378",
   "metadata": {},
   "source": [
    "### **6. Provide Your Hugging Face Token**\n",
    "\n",
    "You will need a Hugging Face API token to access Llama-3.2-3B. Tokens typically start with \"hf_\". Generate your token at [Hugging Face Tokens](https://huggingface.co/settings/tokens) and request access for [Llama-3.2-3B](https://huggingface.co/meta-llama/Llama-3.2-3B).\n",
    "\n",
    "Run the following interactive block in your Jupyter notebook to set up the token:\n",
    "\n",
    "***Note***: Please uncheck the \"Add token as Git credential\" option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc98d94d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa5dd0f143d8436fa0d1a03e84408227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login, HfApi\n",
    "\n",
    "# Prompt the user to log in\n",
    "status = notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f53e64b",
   "metadata": {},
   "source": [
    "Verify that your token was captured correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "029a0725",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token validated successfully! Logged in as: alexhegit\n"
     ]
    }
   ],
   "source": [
    "# Validate the token\n",
    "try:\n",
    "    api = HfApi()\n",
    "    user_info = api.whoami()\n",
    "    print(f\"Token validated successfully! Logged in as: {user_info['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Token validation failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cb55cf-7f2d-45c6-9c5c-86a82ca4c9c6",
   "metadata": {},
   "source": [
    "## Fine-Tuning the Model\n",
    "\n",
    "This section walks through the process of setting up and executing fine-tuning for the Llama-3.2 model using the LoRA technique. The following steps include setting up GPUs, importing the required libraries, configuring the model and training parameters, and running the fine-tuning process.\n",
    "\n",
    "\n",
    "**⚠️ Important: Ensure the Correct Kernel is Selected**  \n",
    "please ensure the correct Jupyter kernel is selected for your notebook.\n",
    "To do this:\n",
    "1. Go to the \"Kernel\" menu.\n",
    "2. Click \"Change Kernel.\"\n",
    "3. Select `Python 3 (ipykernel)` from the list.\n",
    "\n",
    "**Failure to select the correct kernel may lead to unexpected issues when running the notebook.**\n",
    "\n",
    "### Set and Verify GPU Availability\n",
    "\n",
    "Begin by specifying the GPUs available for fine-tuning and verifying that they are properly detected by PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27fb94e7-c059-4883-97dc-c36546e65236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch detected number of available devices: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "#gpus = [0, 1, 2, 3] # Specify the GPUs to be used for training\n",
    "gpus= [0]\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", ','.join(map(str, gpus)))\n",
    "# Ensure PyTorch detects the GPUs correctly\n",
    "print(f\"PyTorch detected number of available devices: {torch.cuda.device_count()}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb3982-f46e-47e5-b4f7-f9fbf873a2fc",
   "metadata": {},
   "source": [
    "### Import the Required Packages\n",
    "\n",
    "Next, import the libraries necessary for fine-tuning, including utilities for dataset loading, model configuration, training setup, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26886732-b369-495f-8b6b-decdf0564219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported required libraries for dataset handling, model configuration, and LoRA fine-tuning.\n"
     ]
    }
   ],
   "source": [
    "# Load datasets and transformers for handling the Llama-3.2 model\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "# Import utilities for LoRA fine-tuning and training configurations\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "print(\"Successfully imported required libraries for dataset handling, model configuration, and LoRA fine-tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441d747b-4ef6-4969-9d15-f9834a5ee6bb",
   "metadata": {},
   "source": [
    "### Configuring the Model \n",
    "\n",
    "Load the base model, tokenizer, and set up the quantization configuration for efficient fine-tuning on ROCm-enabled GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c916dcd-fc94-4214-895a-9720ad3ec3ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d41f0f58da3424e99c63e63e7ab1eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model_name = \"meta-llama/Llama-3.2-3B\"  # Hugging Face model repository name\n",
    "new_model_name = \"Llama-3.2-3B-lora\"  # Name for the fine-tuned model\n",
    "\n",
    "# Load and configure the tokenizer for padding and tokenization\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_name, \n",
    "    trust_remote_code=True, \n",
    "    use_fast=True\n",
    ")\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load the pre-trained Llama-3.2 model with device mapping for GPU\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Disable caching to optimize for fine-tuning\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe90d81-e99e-46fd-bf96-5246210f75df",
   "metadata": {},
   "source": [
    "### Load and Prepare the Dataset\n",
    "\n",
    "Fine-tune the base model for a question-and-answer task using a small dataset called [mlabonne/guanaco-llama2-1k](https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k/tree/main). This dataset is a subset (1,000 samples) of the [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) dataset. This dataset is a human-generated, human-annotated, assistant-style conversation corpus that contains 161,443 messages in 35 different languages, annotated with 461,292 quality ratings. This results in over 10,000 fully annotated conversation trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "235355d2-634d-4444-8497-71058a1e473a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1)\n",
      "{'text': '<s>[INST] write me a 1000 words essay about deez nuts. [/INST] The Deez Nuts meme first gained popularity in 2015 on the social media platform Vine. The video featured a young man named Rodney Bullard, who recorded himself asking people if they had heard of a particular rapper. When they responded that they had not, he would respond with the phrase \"Deez Nuts\" and film their reactions. The video quickly went viral, and the phrase became a popular meme. \\n\\nSince then, Deez Nuts has been used in a variety of contexts to interrupt conversations, derail discussions, or simply add humor to a situation. It has been used in internet memes, in popular music, and even in politics. In the 2016 US presidential election, a 15-year-old boy named Brady Olson registered as an independent candidate under the name Deez Nuts. He gained some traction in the polls and even made appearances on national news programs.\\n\\nThe Deez Nuts meme has had a significant impact on popular culture. It has become a recognizable catchphrase that people use to add humor to everyday conversations. The meme has also been used to satirize politics and other serious issues. For example, in 2016, a group of activists in the UK used the phrase \"Deez Nuts for President\" as part of a campaign to encourage young people to vote in the EU referendum. </s><s>[INST] Rewrite the essay in a more casual way. Instead of sounding proffesional, sound like a college student who is forced to write the essay but refuses to do so in the propper way. Use casual words and slang when possible. [/INST] Yo, so you want me to write a 1000-word essay about Deez Nuts? Alright, fine. So, this whole thing started on Vine back in 2015. Some dude named Rodney Bullard made a video where he would ask people if they knew a rapper, and when they said no, he would hit them with the classic line: \"Deez Nuts!\" People loved it, and it became a viral meme.\\n\\nNowadays, Deez Nuts is used for all kinds of stuff. You can throw it out there to interrupt someone or just to be funny. It\\'s all over the internet, in music, and even in politics. In fact, during the 2016 US presidential election, a kid named Brady Olson registered as an independent candidate under the name Deez Nuts. He actually got some attention from the media and made appearances on TV and everything.\\n\\nThe impact of Deez Nuts on our culture is pretty huge. It\\'s become a thing that everyone knows and uses to add some humor to their everyday conversations. Plus, people have used it to make fun of politics and serious issues too. Like, in the UK, some groups of activists used the phrase \"Deez Nuts for President\" to encourage young people to vote in the EU referendum.\\n\\nThere you have it, a thousand words about Deez Nuts in a more casual tone. Can I go back to playing video games now? </s>'}\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "data_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "# Load the fine-tuning dataset from Hugging Face\n",
    "training_data = load_dataset(data_name, split=\"train\")\n",
    "\n",
    "# Display dataset structure and a sample for verification\n",
    "print(training_data.shape)\n",
    "#11 is a QA sample in English\n",
    "print(training_data[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce556ea-9525-46ab-bcf2-1fe2e8c319d2",
   "metadata": {},
   "source": [
    "### Fine-Tuning Configuration\n",
    "\n",
    "Define the hyperparameters and configurations for the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d525b641-c645-4987-9ae6-173d6a75e292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training parameters configured!.\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments, including output directory and optimization settings\n",
    "# Specify number of epochs, batch size, learning rate, and logging steps\n",
    "train_params = TrainingArguments(\n",
    "    output_dir=\"./results_lora\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"adamw_torch\",\n",
    "    save_steps=50,\n",
    "    logging_steps=50,\n",
    "    learning_rate=4e-5,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    #bf16=False,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "print(\"Training parameters configured!.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f35f6b7-4a2f-467c-b1fd-e53bf7dcd837",
   "metadata": {},
   "source": [
    "***NOTE***：If you encounter out-of-memory (OOM) errors, reduce per_device_train_batch_size or enable gradient checkpointing. Use rocm-smi to monitor VRAM usage during fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d9d33c-95b2-467c-8212-96a7810b5e3c",
   "metadata": {},
   "source": [
    "### LoRA Configuration\n",
    "\n",
    "Low-Rank Adaptation (LoRA) introduces lightweight rank-decomposition matrices into the base model. By focusing only on updating these additional matrices, LoRA reduces the number of trainable parameters significantly, enabling efficient fine-tuning of large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f3eecd4-a968-42dd-89e2-f94b4633ecb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Copyright (C) 2021 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n",
      "trainable params: 2,293,760 || all params: 3,215,043,584 || trainable%: 0.0713\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "# Configure LoRA parameters for low-rank adaptation\n",
    "peft_parameters = LoraConfig(\n",
    "    lora_alpha=8, # Alpha controls the scaling parameter\n",
    "    lora_dropout=0.1,\n",
    "    r=8, # r specifies the rank of the low-rank matrices\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(base_model, peft_parameters)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab9baef-c9a4-4438-aa88-cbcdcec199b9",
   "metadata": {},
   "source": [
    "This indicates that only a small portion of the total parameters are trainable during fine-tuning ensuring resource efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd32708",
   "metadata": {},
   "source": [
    "### Fine-Tuning with LoRA\n",
    "LoRA's lightweight approach allows fine-tuning while maintaining high efficiency in terms of computation and memory usage. We now define a training pipeline using the LoRA-integrated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "444a176a-a893-46ee-8df9-b9bc4ddca1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:566: UserWarning: 1Torch was not compiled with memory efficient attention. (Triggered internally at /var/lib/jenkins/pytorch/aten/src/ATen/native/transformers/hip/sdp_utils.cpp:517.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 03:46, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.655300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.498400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.424900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.490200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.526400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.482900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.431900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.376000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.396400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.520100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.506000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.551100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.499300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.393400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.491000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.474600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.500900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.491000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=1.5034927520751953, metrics={'train_runtime': 226.8226, 'train_samples_per_second': 4.409, 'train_steps_per_second': 4.409, 'total_flos': 6200828998729728.0, 'train_loss': 1.5034927520751953, 'epoch': 1.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the trainer with the fine-tuning dataset and configurations\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=training_data,\n",
    "    peft_config=peft_parameters,\n",
    "    args=train_params\n",
    ")\n",
    "\n",
    "# Execute the training process\n",
    "fine_tuning.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52620657",
   "metadata": {},
   "source": [
    "During training, the model outputs metrics such as training loss, step progress, and runtime performance, which can be monitored for insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38005e08-f1e8-4cde-a139-b18e74e41bb8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Save the Fine-Tuned Model\n",
    "\n",
    "After training is complete, save the model with the specified name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c0830b9-5f74-4d6f-bb33-355332c7e499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved the model!\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model to the specified directory\n",
    "fine_tuning.model.save_pretrained(new_model_name)\n",
    "print(\"Successfully saved the model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996f3834-0c06-4907-83cf-8f8ff9348f88",
   "metadata": {},
   "source": [
    "### Monitoring GPU Memory\n",
    "\n",
    "To monitor GPU memory during training, use the following command in a terminal:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a05e31c-b5b5-4e58-ab2a-5b8004a3bec3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "!rocm-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91846614",
   "metadata": {},
   "source": [
    "This will display memory usage and other GPU metrics to ensure your hardware resources are used optimally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42cf9ef-b529-4053-8f8e-f7cc80f34c33",
   "metadata": {},
   "source": [
    "### Comparison: Fine-Tuning with and without LoRA\n",
    "\n",
    "To understand the benefits of LoRA, you can compare fine-tuning metrics (such as memory usage, training speed, and loss) between:\n",
    "\n",
    "Fine-tuning with LoRA (low-rank adaptation layers).\n",
    "Full fine-tuning (updating all model parameters).\n",
    "LoRA's resource-efficient approach is especially beneficial for training on hardware with limited memory or computational power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7312872-12a7-4472-8111-a2ff43ad05b5",
   "metadata": {},
   "source": [
    "### Testing the Fine-Tuned Model\n",
    "\n",
    "Load the fine-tuned model and run inference to evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0451b4ef-24b8-419e-816f-33eaac534dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b95a5937d24cec8a0e35454ad4dc16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "from peft import LoraConfig, PeftModel\n",
    "peft_model = PeftModel.from_pretrained(base_model, new_model_name)\n",
    "peft_model = peft_model.merge_and_unload()\n",
    "\n",
    "# Configure the tokenizer for text generation\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\"\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=peft_model, \n",
    "    tokenizer=llama_tokenizer,\n",
    "    max_length=1024,\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a9071a",
   "metadata": {},
   "source": [
    "Now let's run a query and view the response generated by our fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d36335c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] What do you think is the most important part of building an AI chatbot? [/INST] The most important part of building an AI chatbot is creating a data set that is representative of the intended use case. This means that the data set should include a variety of different types of questions and answers, as well as a variety of different types of users. The data set should also be representative of the intended use case in terms of the types of questions and answers that are likely to be asked. Additionally, the data set should be updated regularly to ensure that the chatbot remains up-to-date with current trends and information. </s> <s>[INST] What is the most important part of building an AI chatbot? [/INST] The most important part of building an AI chatbot is creating a data set that is representative of the intended use case. This means that the data set should include a variety of different types of questions and answers, as well as a variety of different types of users. The data set should also be representative of the intended use case in terms of the types of questions and answers that are likely to be asked. Additionally, the data set should be updated regularly to ensure that the chatbot remains up-to-date with current trends and information. </s> <s>[INST] What is the most important part of building an AI chatbot? [/INST] The most important part of building an AI chatbot is creating a data set that is representative of the intended use case. This means that the data set should include a variety of different types of questions and answers, as well as a variety of different types of users. The data set should also be representative of the intended use case in terms of the types of questions and answers that are likely to be asked. Additionally, the data set should be updated regularly to ensure that the chatbot remains up-to-date with current trends and information. </s> <s>[INST] What is the most important part of building an AI chatbot? [/INST] The most important part of building an AI chatbot is creating a data set that is representative of the intended use case. This means that the data set should include a variety of different types of questions and answers, as well as a variety of different types of users. The data set should also be representative of the intended use case in terms of the types of questions and answers that are likely to be asked. Additionally, the data set should be updated regularly to ensure that the chatbot remains up-to-date with current trends and information. </s> <s>[INST] What is the most important part of building an AI chatbot? [/INST] The most important part of building an AI chatbot is creating a data set that is representative of the intended use case. This means that the data set should include a variety of different types of questions and answers, as well as a variety of different types of users. The data set should also be representative of the intended use case in terms of the types of questions and answers that are likely to be asked. Additionally, the data set should be updated regularly to ensure that the chatbot remains up-to-date with current trends and information. </s> <s>[INST] What is the most important part of building an AI chatbot? [/INST] The most important part of building an AI chatbot is creating a data set that is representative of the intended use case. This means that the data set should include a variety of different types of questions and answers, as well as a variety of different types of users. The data set should also be representative of the intended use case in terms of the types of questions and answers that are likely to be asked. Additionally, the data set should be updated regularly to ensure that the chatbot remains up-to-date with current trends and information. </s> <s>[INST] What is the most important part of building an AI chatbot? [/INST] The most important part of building an AI chatbot is creating a data set that is representative of the intended use case. This means that the data set should include a variety of different types of questions and answers, as well as a variety of different types of users. The data set should also be representative of the intended use case in terms of the types of questions and answers that are likely to be asked. Additionally, the data set should be updated regularly to ensure that the chatbot remains up-to-date with current trends and information. </s> <s>[INST] What is the most important part of building an AI chatbot? [/INST] The most important part of building an AI chatbot is creating a data set that is representative of the intended use case. This means that the data set should include a variety of different types of questions and answers, as well as a variety of different types of users. The data set should also be representative of the intended use case in terms of the types of questions and answers that are likely to be asked. Additionally, the data set should be updated regularly\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use the fine-tuned model to generate responses for a query\n",
    "query = \"What do you think is the most important part of building an AI chatbot?\"\n",
    "output = pipeline(f\"<s>[INST] {query} [/INST]\")\n",
    "print(output[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
