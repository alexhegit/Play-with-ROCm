{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c73b0caa-096b-45fe-b26f-032128d4334f",
   "metadata": {},
   "source": [
    "# Fine-tune Llama-3.1 with LoRA with AMD ROCm GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d94b31-35f8-4c8c-af0a-8a10aa5b4c62",
   "metadata": {},
   "source": [
    "In this blog, we show you how to fine-tune Llama-3.1-8B on AMD GPU with ROCm. We use Low-Rank Adaptation of Large Language Models (LoRA) to overcome memory and computing limitations and make open-source large language models (LLMs) more accessible.\n",
    "\n",
    "## Step-by-step fine-tuning\n",
    "\n",
    "Standard (full-parameter) fine-tuning involves considering all parameters. It requires significant computational power to manage optimizer states and gradient check-pointing. The resulting memory footprint is typically about four times larger than the model itself.\n",
    "\n",
    "To overcome this memory limitation, you can use a parameter-efficient fine-tuning (PEFT) technique, such as LoRA.\n",
    "\n",
    "\n",
    "Our setup:\n",
    "\n",
    "- Hardware: AMD ROCm GPU (MI325X, MI300X, etc) [device list](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html)\n",
    "- Software:\n",
    "    - ROCm 6.0+\n",
    "    - Pytorch 2.0.1+\n",
    "    - Libraries: transformers, accelerate, peft, trl, bitsandbytes, scipy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4926a00e-7805-4de6-bb72-43db16ac09a2",
   "metadata": {},
   "source": [
    "### Step 0: Setup ROCm environment\n",
    "\n",
    "The easyway is to use ROCm docker image from https://hub.docker.com/r/rocm/pytorch. I use TAG `rocm/pytorch:rocm6.1_ubuntu22.04_py3.10_pytorch_2.1.2`.\n",
    "\n",
    "```bash\n",
    "$docker pull rocm/pytorch:rocm6.1_ubuntu22.04_py3.10_pytorch_2.1.2\n",
    "```\n",
    "\n",
    "And here is my docker start command as your reference.\n",
    "\n",
    "```bash\n",
    "$alias drun='docker run -it --network=host --device=/dev/kfd --device=/dev/dri --group-add=video --ipc=host --cap-add=SYS_PTRACE --security-opt seccomp=unconfined --shm-size 8G --hostname=ROCm-FT -v /DATA:/DATA -w /DATA'\n",
    "\n",
    "$drun rocm/pytorch:rocm6.1_ubuntu22.04_py3.10_pytorch_2.1.2\n",
    "```\n",
    "\n",
    "To run this jupyter notebook, you may install it by `pip install jupyter-lab`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b72aff-616a-442c-91a2-b9974ab95255",
   "metadata": {},
   "source": [
    "### Step 1: Getting started\n",
    "\n",
    "First, let’s confirm the availability of the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983ea9f6-1d12-4d99-9135-00d79e4dd1ef",
   "metadata": {},
   "source": [
    "Next, install the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f4a7d77-c810-44ba-b70d-4661139f1cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============================ ROCm System Management Interface ============================\n",
      "====================================== Product Info ======================================\n",
      "GPU[0]\t\t: Card series: \t\tInstinct MI210\n",
      "GPU[0]\t\t: Card model: \t\t0x0c34\n",
      "GPU[0]\t\t: Card vendor: \t\tAdvanced Micro Devices, Inc. [AMD/ATI]\n",
      "GPU[0]\t\t: Card SKU: \t\tD67301V\n",
      "GPU[1]\t\t: Card series: \t\tInstinct MI210\n",
      "GPU[1]\t\t: Card model: \t\t0x0c34\n",
      "GPU[1]\t\t: Card vendor: \t\tAdvanced Micro Devices, Inc. [AMD/ATI]\n",
      "GPU[1]\t\t: Card SKU: \t\tD67301V\n",
      "GPU[2]\t\t: Card series: \t\tInstinct MI210\n",
      "GPU[2]\t\t: Card model: \t\t0x0c34\n",
      "GPU[2]\t\t: Card vendor: \t\tAdvanced Micro Devices, Inc. [AMD/ATI]\n",
      "GPU[2]\t\t: Card SKU: \t\tD67301V\n",
      "GPU[3]\t\t: Card series: \t\tInstinct MI210\n",
      "GPU[3]\t\t: Card model: \t\t0x0c34\n",
      "GPU[3]\t\t: Card vendor: \t\tAdvanced Micro Devices, Inc. [AMD/ATI]\n",
      "GPU[3]\t\t: Card SKU: \t\tD67301V\n",
      "==========================================================================================\n",
      "================================== End of ROCm SMI Log ===================================\n"
     ]
    }
   ],
   "source": [
    "!rocm-smi --showproductname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaedeb92-bf10-4bbc-967c-57e14db7fc8b",
   "metadata": {},
   "source": [
    "!pip install -q pandas peft==0.14.0 transformers==4.47.1 trl==0.13.0 accelerate==1.2.1 scipy tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f85ff88-a4d5-4685-b1a2-96dcd48fc653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft                       0.14.0\n",
      "transformers               4.47.1\n",
      "accelerate                 1.2.1\n",
      "trl                        0.13.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip list | grep peft\n",
    "pip list | grep transformer\n",
    "pip list | grep accelerate\n",
    "pip list | grep trl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e8dc50-0464-44c5-8931-0a6dfe812f79",
   "metadata": {},
   "source": [
    "#### Install bitsandbytes\n",
    "1. Install bitsandbytes using the following code.\n",
    "\n",
    "- For ROCm 6.2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d1640f6-b63e-4041-9e77-fff1bc23550d",
   "metadata": {},
   "source": [
    "# Install `bitsandbytes`\n",
    "git clone --recurse https://github.com/ROCm/bitsandbytes.git\n",
    "cd bitsandbytes\n",
    "git checkout rocm6.2_internal_testing\n",
    "make hip\n",
    "python setup.py install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a0abf0-afad-4cc7-a46c-f832f107e1d7",
   "metadata": {},
   "source": [
    "2. Check the bitsandbytes version (0.42.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd5e772a-bfcb-4c7f-9457-dc22f631b6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitsandbytes               0.42.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip list | grep bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cb55cf-7f2d-45c6-9c5c-86a82ca4c9c6",
   "metadata": {},
   "source": [
    "#### Check and Set GPUs for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27fb94e7-c059-4883-97dc-c36546e65236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch detected number of availabel devices: 4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "# set visible gpus as need\n",
    "gpus = [0, 1, 2, 3]\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", ','.join(map(str, gpus)))\n",
    "print(f\"PyTorch detected number of availabel devices: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb3982-f46e-47e5-b4f7-f9fbf873a2fc",
   "metadata": {},
   "source": [
    "#### Import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26886732-b369-495f-8b6b-decdf0564219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441d747b-4ef6-4969-9d15-f9834a5ee6bb",
   "metadata": {},
   "source": [
    "### Step 2: Configuring the model and data\n",
    "Please make sure the LLM model files has been download and use the real path in the below code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c916dcd-fc94-4214-895a-9720ad3ec3ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `SDPA` attention implementation on multi-gpu setup with ROCM may lead to performance issues due to the FA backend. Disabling it to use alternative backends.\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:04<00:00,  1.21s/it]\n"
     ]
    }
   ],
   "source": [
    "# Model and tokenizer names\n",
    "base_model_name = \"/data/HF-MODEL/huggingface-model/Meta-Llama-3.1-8B/\"\n",
    "new_model_name = \"Llama-3.1-8B-lora\" #You can give your own name for fine tuned model\n",
    "\n",
    "# Tokenizer\n",
    "#llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True, use_fast=True)\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "      load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=\"float16\", bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# Model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe90d81-e99e-46fd-bf96-5246210f75df",
   "metadata": {},
   "source": [
    "After you have the base model, you can start fine-tuning. We fine-tune our base model for a question-and-answer task using a small data set called mlabonne/guanaco-llama2-1k, which is a subset (1,000 samples) of the timdettmers/openassistant-guanaco data set. This data set is a human-generated, human-annotated, assistant-style conversation corpus that contains 161,443 messages in 35 different languages, annotated with 461,292 quality ratings. This results in over 10,000 fully annotated conversation trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "235355d2-634d-4444-8497-71058a1e473a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1)\n",
      "{'text': '<s>[INST] write me a 1000 words essay about deez nuts. [/INST] The Deez Nuts meme first gained popularity in 2015 on the social media platform Vine. The video featured a young man named Rodney Bullard, who recorded himself asking people if they had heard of a particular rapper. When they responded that they had not, he would respond with the phrase \"Deez Nuts\" and film their reactions. The video quickly went viral, and the phrase became a popular meme. \\n\\nSince then, Deez Nuts has been used in a variety of contexts to interrupt conversations, derail discussions, or simply add humor to a situation. It has been used in internet memes, in popular music, and even in politics. In the 2016 US presidential election, a 15-year-old boy named Brady Olson registered as an independent candidate under the name Deez Nuts. He gained some traction in the polls and even made appearances on national news programs.\\n\\nThe Deez Nuts meme has had a significant impact on popular culture. It has become a recognizable catchphrase that people use to add humor to everyday conversations. The meme has also been used to satirize politics and other serious issues. For example, in 2016, a group of activists in the UK used the phrase \"Deez Nuts for President\" as part of a campaign to encourage young people to vote in the EU referendum. </s><s>[INST] Rewrite the essay in a more casual way. Instead of sounding proffesional, sound like a college student who is forced to write the essay but refuses to do so in the propper way. Use casual words and slang when possible. [/INST] Yo, so you want me to write a 1000-word essay about Deez Nuts? Alright, fine. So, this whole thing started on Vine back in 2015. Some dude named Rodney Bullard made a video where he would ask people if they knew a rapper, and when they said no, he would hit them with the classic line: \"Deez Nuts!\" People loved it, and it became a viral meme.\\n\\nNowadays, Deez Nuts is used for all kinds of stuff. You can throw it out there to interrupt someone or just to be funny. It\\'s all over the internet, in music, and even in politics. In fact, during the 2016 US presidential election, a kid named Brady Olson registered as an independent candidate under the name Deez Nuts. He actually got some attention from the media and made appearances on TV and everything.\\n\\nThe impact of Deez Nuts on our culture is pretty huge. It\\'s become a thing that everyone knows and uses to add some humor to their everyday conversations. Plus, people have used it to make fun of politics and serious issues too. Like, in the UK, some groups of activists used the phrase \"Deez Nuts for President\" to encourage young people to vote in the EU referendum.\\n\\nThere you have it, a thousand words about Deez Nuts in a more casual tone. Can I go back to playing video games now? </s>'}\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "data_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "training_data = load_dataset(data_name, split=\"train\")\n",
    "# check the data\n",
    "print(training_data.shape)\n",
    "# #11 is a QA sample in English\n",
    "print(training_data[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce556ea-9525-46ab-bcf2-1fe2e8c319d2",
   "metadata": {},
   "source": [
    "### Step 3: Start fine-tuning\n",
    "To set your training parameters, use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d525b641-c645-4987-9ae6-173d6a75e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Params\n",
    "train_params = TrainingArguments(\n",
    "    output_dir=\"./results_lora\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=50,\n",
    "    logging_steps=50,\n",
    "    learning_rate=4e-5,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f35f6b7-4a2f-467c-b1fd-e53bf7dcd837",
   "metadata": {},
   "source": [
    "**NOTE**：You may decrease the per_device_train_batch_size if got OOM. Use rocm-smi to monitor the VRAM usage when running the finetuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d9d33c-95b2-467c-8212-96a7810b5e3c",
   "metadata": {},
   "source": [
    "**Training with LoRA configuration**\n",
    "\n",
    "Now you can integrate LoRA into the base model and assess its additional parameters. LoRA essentially adds pairs of rank-decomposition weight matrices (called update matrices) to existing weights, and only trains the newly added weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f3eecd4-a968-42dd-89e2-f94b4633ecb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "# LoRA Config\n",
    "peft_parameters = LoraConfig(\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(base_model, peft_parameters)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab9baef-c9a4-4438-aa88-cbcdcec199b9",
   "metadata": {},
   "source": [
    "Above show the trainalbe parameters in percent which is a tiny portion of the original model. This is the percentage we’ll update through fine-tuning, as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "444a176a-a893-46ee-8df9-b9bc4ddca1cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 09:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.653700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.422100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.336700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.387400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.383500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=1.4366847839355468, metrics={'train_runtime': 550.3139, 'train_samples_per_second': 1.817, 'train_steps_per_second': 0.454, 'total_flos': 1.6854644828110848e+16, 'train_loss': 1.4366847839355468, 'epoch': 1.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainer with LoRA configuration\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=training_data,\n",
    "    peft_config=peft_parameters,\n",
    "    #dataset_text_field=\"text\",\n",
    "    #tokenizer=llama_tokenizer,\n",
    "    args=train_params\n",
    ")\n",
    "\n",
    "# Training\n",
    "fine_tuning.train()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "38005e08-f1e8-4cde-a139-b18e74e41bb8",
   "metadata": {},
   "source": [
    "To save your model, run this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c0830b9-5f74-4d6f-bb33-355332c7e499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "fine_tuning.model.save_pretrained(new_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996f3834-0c06-4907-83cf-8f8ff9348f88",
   "metadata": {},
   "source": [
    "#### Checking memory usage during training with LoRA\n",
    "During training, you can check the memory usage by running the rocm-smi command in a terminal. This command produces the following output:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a05e31c-b5b5-4e58-ab2a-5b8004a3bec3",
   "metadata": {},
   "source": [
    "$rocm-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42cf9ef-b529-4053-8f8e-f7cc80f34c33",
   "metadata": {},
   "source": [
    "To facilitate a comparison between fine-tuning with and without LoRA, our subsequent phase involves running a thorough fine-tuning process on the base model. This involves updating all parameters within the base model. We then analyze differences in memory usage, training speed, training loss, and other relevant metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7312872-12a7-4472-8111-a2ff43ad05b5",
   "metadata": {},
   "source": [
    "### Step 4: Test the fine-tuned model with LoRA\n",
    "\n",
    "To test your model, run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4912dbf-3670-495e-b630-8d8474338e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419366ea-1aaf-4648-8350-bb2eea033c03",
   "metadata": {},
   "source": [
    "#### Fine-tuned Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0451b4ef-24b8-419e-816f-33eaac534dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "\n",
    "#base_model_name = \"/data/HF-MODEL/huggingface-model/Meta-Llama-3.1-8B/\"\n",
    "#new_model_name = \"Llama-3.1-8B-lora\" #You can give your own name for fine tuned model\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "from peft import LoraConfig, PeftModel\n",
    "peft_model = PeftModel.from_pretrained(base_model, new_model_name)\n",
    "peft_model = peft_model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10ff3f8f-24a1-42f9-82e2-200e74da6966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipeline = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=peft_model, \n",
    "    tokenizer=llama_tokenizer,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f4f91f6-5508-4f80-9bad-658c84141411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] What do you think is the most important part of building an AI chatbot? [/INST] There are many different aspects to consider when building an AI chatbot, but I believe that the most\n"
     ]
    }
   ],
   "source": [
    "query = \"What do you think is the most important part of building an AI chatbot?\"\n",
    "output = pipeline(f\"<s>[INST] {query} [/INST]\")\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6abb4ea-93bb-4e24-beef-57a080501aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
